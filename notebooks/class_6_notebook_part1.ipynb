{"cells":[{"cell_type":"markdown","metadata":{"id":"pxHMrtesfjhm"},"source":["# **Class 6: Part 1 - PyTorch Advanced**"]},{"cell_type":"markdown","metadata":{"id":"0EmpiOb96l4J"},"source":["## Understanding `torch.nn.Module`\n","\n","The `torch.nn.Module` class is a fundamental building block in PyTorch's neural network library. It provides a framework to create, organize, and manage neural network layers, enabling developers to define custom architectures with ease. Mastering `nn.Module` is crucial for building and customizing neural networks in PyTorch.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8oyKcrrdfqqy"},"source":["### What is `torch.nn.Module`?\n","\n","`torch.nn.Module` serves as the base class for all neural network modules in PyTorch. It acts as a container for various layers and operations, offering essential functionalities such as parameter management, device assignment (e.g., GPU/CPU), and model persistence (saving/loading).\n"]},{"cell_type":"markdown","metadata":{"id":"E34ZPReLfsgN"},"source":["\n","### Purpose of `nn.Module`\n","\n","- **Encapsulation**: Encapsulates network layers and their associated behaviors within a reusable, organized class structure.\n","- **Parameter Management**: Automatically registers and manages parameters (e.g., weights and biases) for each layer in the module.\n","- **Forward Propagation**: Defines the computation that occurs during the forward pass through the network.\n","- **Utility Functions**: Provides helpful methods such as `cuda()` and `cpu()` for device handling and `state_dict()` and `load_state_dict()` for serialization and deserialization of model parameters."]},{"cell_type":"markdown","metadata":{"id":"DU2-2nOv6n-K"},"source":["\n","## Core Methods of `torch.nn.Module`\n","\n","### `__init__()`\n","The constructor method where you define and initialize your layers and other components of the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d74hB3rm64SB"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","\n","# Define the logistic regression model\n","class LogisticRegressionModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super(LogisticRegressionModel, self).__init__()\n","        # Define a single linear layer (Logistic Regression)\n","        self.linear = nn.Linear(input_dim, 1)  # Single output for binary classification\n"]},{"cell_type":"markdown","metadata":{"id":"xZCpB7RU7CBb"},"source":["### `forward(input)`\n","\n","Defines the logic for the forward pass, specifying how input data flows through layers to produce the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q13n89hl69T7"},"outputs":[],"source":["import torch.nn as nn\n","\n","# Define the logistic regression model\n","class LogisticRegressionModel(nn.Module):\n","    def __init__(self, input_dim):\n","        super(LogisticRegressionModel, self).__init__()\n","        # Define a single linear layer (Logistic Regression)\n","        self.linear = nn.Linear(input_dim, 1)  # Single output for binary classification\n","\n","    def forward(self, x):\n","        # Forward pass: apply the linear layer and sigmoid activation\n","        return torch.sigmoid(self.linear(x))"]},{"cell_type":"markdown","metadata":{"id":"Ik5q_2Qo7KYu"},"source":["### `parameters()`\n","Returns an iterator over the module’s parameters (e.g., weights and biases). Typically used when passing model parameters to optimizers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNc8wX3A69Md"},"outputs":[],"source":["model = LogisticRegressionModel(100)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"]},{"cell_type":"markdown","metadata":{"id":"5yl5GgIY7rip"},"source":["### `named_parameters()`\n","Similar to `parameters()`, but returns a generator that yields both the name and the parameter tensor. Useful for inspecting or differentiating between layers based on their names."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1728995163584,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"Gy3nwiE969Et","outputId":"6c1830f0-6b57-41e0-f5f6-d660259c213f"},"outputs":[],"source":["for name, param in model.named_parameters():\n","    print(f\"Layer: {name}, Param: {param.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"JtR-_HAg73bT"},"source":["### `state_dict()`\n","\n","Returns a dictionary containing the state of the module, including model parameters and persistent buffers (e.g., running averages for batch normalization). Used to save the model’s parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1728995163584,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"4P-_VJ0i7ztO","outputId":"2acddcab-ff44-47a8-9efd-107a8f391ea4"},"outputs":[],"source":["model.state_dict()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtJNTeIr7zmN"},"outputs":[],"source":["torch.save(model.state_dict(), \"model.pth\")  # Save model state"]},{"cell_type":"markdown","metadata":{"id":"aSHmQNDW8AMF"},"source":["### `load_state_dict(state_dict)`\n","\n","Loads the model parameters from a dictionary (typically obtained from `state_dict()`). This is used to restore a model from a saved state."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1728995163584,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"N1QkruKL7zfN","outputId":"38d7fa3f-e021-4120-bc00-9ce7a9610bfe"},"outputs":[],"source":["model.load_state_dict(torch.load(\"model.pth\"))  # Load saved state"]},{"cell_type":"markdown","metadata":{"id":"AqTl6eAs8OJ5"},"source":["### `train(mode=True)`\n","\n","Sets the module in training mode, affecting certain layers like Dropout and BatchNorm which behave differently during training and evaluation. By default,`mode=True` sets the module to training mode. Pass `mode=False` to set it to evaluation mode."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1728995163585,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"XqpLb-257zYM","outputId":"2b9ad68e-0219-4435-9a7a-2c9a0dc8830f"},"outputs":[],"source":["model.train()  # Set model to training mode"]},{"cell_type":"markdown","metadata":{"id":"0d91-2tL8jG_"},"source":["### `eval()`\n","\n","A shorthand for `train(False)`, setting the module in evaluation mode. This is crucial for ensuring the correct behavior of certain layers, such as disabling dropout and using running averages for batch normalization."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":394,"status":"ok","timestamp":1728995163975,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"AElooDT67zRN","outputId":"38d3aeb4-f402-4df8-eca2-0e2ef899b803"},"outputs":[],"source":["model.eval()  # Set model to evaluation mode"]},{"cell_type":"markdown","metadata":{"id":"rVfhndGP8pYl"},"source":["### `to(device)`\n","\n","Moves the module and its parameters to a specified device (e.g., CPU or GPU). This is important for using GPUs to accelerate computations."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728995163975,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"aQQu1y6n7zKf","outputId":"3a02bd25-7481-4940-8a18-5bef72c7f46d"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)  # Move model to GPU if available"]},{"cell_type":"markdown","metadata":{"id":"auq48LX19CNg"},"source":["## How it Works\n","\n","When you create a custom network by inheriting from nn.Module, you define the layers in the`__init__` method and implement the forward pass logic in the forward method. The forward method is automatically called when the module is used as a function, facilitating easy integration of custom networks into PyTorch workflows.\n","\n","By using these core methods and utilities, you can build sophisticated neural network architectures, manage model parameters efficiently, and deploy models on different devices seamlessly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a8FVEUY-7y8g"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5YwqP54688w"},"outputs":[],"source":["# Create synthetic data for demonstration\n","X, y = make_classification(n_samples=1000, n_features=2, n_classes=2, n_redundant=0, random_state=42)\n","X = StandardScaler().fit_transform(X)  # Normalize features\n","y = y.reshape(-1, 1)  # Reshape to be a column vector\n","\n","# Split data into train, dev (validation), and test sets\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% train, 30% temp\n","X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split temp into 15% dev and 15% test\n","\n","# Convert data to PyTorch tensors\n","X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n","y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n","X_dev_tensor = torch.tensor(X_dev, dtype=torch.float32)\n","y_dev_tensor = torch.tensor(y_dev, dtype=torch.float32)\n","X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6yIuYo_9dJf"},"outputs":[],"source":["# Instantiate the model\n","\n","# Define loss function and optimizer\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19332,"status":"ok","timestamp":1728995186444,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"hSqYwitS9nU3","outputId":"151f22ba-0639-4d33-951d-a47ffcd2c17b"},"outputs":[],"source":["torch.manual_seed(42)\n","num_epochs = 10000\n","train_losses = []\n","dev_losses = []\n","\n","for epoch in range(num_epochs):\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"elapsed":757,"status":"ok","timestamp":1728995187197,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"kUhmtFsq_O70","outputId":"5ea52b13-2145-4c4b-b45b-ff7659248f06"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Plotting the training and validation loss curves\n","plt.figure(figsize=(10, 5))\n","plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n","plt.plot(range(1, num_epochs + 1), dev_losses, label='Dev Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training and Dev Loss over Epochs')\n","plt.legend()\n","plt.grid()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1728995187197,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"enjwt_en-Abv","outputId":"f6d46bdf-7eb8-4131-c32d-df2d17b0a3e1"},"outputs":[],"source":["# Final Test Set Evaluation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SPrDaoJRqBPH"},"source":["## What is a classification problem?\n","\n","A [classification problem](https://en.wikipedia.org/wiki/Statistical_classification) involves predicting whether something is one thing or another.\n","\n","For example, you might want to:\n","\n","| Problem type | What is it? | Example |\n","| ----- | ----- | ----- |\n","| **Binary classification** | Target can be one of two options, e.g. yes or no | Predict whether or not someone has heart disease based on their health parameters. |\n","| **Multi-class classification** | Target can be one of more than two options | Decide whether a photo is of food, a person or a dog. |\n","| **Multi-label classification** | Target can be assigned more than one option | Predict what categories should be assigned to a Wikipedia article (e.g. mathematics, science & philosophy). |\n","\n","<div align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-different-classification-problems.png\" alt=\"various different classification in machine learning such as binary classification, multiclass classification and multilabel classification\" width=900/>\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"e-8Qust6q_2r"},"source":["\n","Before we get into writing code, let's look at the general architecture of a classification neural network.\n","\n","| **Hyperparameter** | **Binary Classification** | **Multiclass classification** |\n","| --- | --- | --- |\n","| **Input layer shape** (`in_features`) | Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) | Same as binary classification |\n","| **Hidden layer(s)** | Problem specific, minimum = 1, maximum = unlimited | Same as binary classification |\n","| **Neurons per hidden layer** | Problem specific, generally 10 to 512 | Same as binary classification |\n","| **Output layer shape** (`out_features`) | 1 (one class or the other) | 1 per class (e.g. 3 for food, person or dog photo) |\n","| **Hidden layer activation** | Usually [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) (rectified linear unit) but [can be many others](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions) | Same as binary classification |\n","| **Output activation** | [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) ([`torch.sigmoid`](https://pytorch.org/docs/stable/generated/torch.sigmoid.html) in PyTorch)| [Softmax](https://en.wikipedia.org/wiki/Softmax_function) ([`torch.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) in PyTorch) |\n","| **Loss function** | [Binary crossentropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) ([`torch.nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) in PyTorch) | Cross entropy ([`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) in PyTorch) |\n","| **Optimizer** | [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) (stochastic gradient descent), [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) (see [`torch.optim`](https://pytorch.org/docs/stable/optim.html) for more options) | Same as binary classification |\n","\n","Of course, this ingredient list of classification neural network components will vary depending on the problem you're working on."]},{"cell_type":"markdown","metadata":{"id":"nkbvBiGSrYZE"},"source":["## Building a multi-class PyTorch model\n","\n","A **binary classification** problem deals with classifying something as one of two options (e.g. a photo as a cat photo or a dog photo) where as a **multi-class classification** problem deals with classifying something from a list of *more than* two options (e.g. classifying a photo as a cat a dog or a chicken).\n","\n","![binary vs multi-class classification image with the example of dog vs cat for binary classification and dog vs cat vs chicken for multi-class classification](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-binary-vs-multi-class-classification.png)\n","*Example of binary vs. multi-class classification. Binary deals with two classes (one thing or another), where as multi-class classification can deal with any number of classes over two, for example, the popular [ImageNet-1k dataset](https://www.image-net.org/) is used as a computer vision benchmark and has 1000 classes.*\n"]},{"cell_type":"markdown","metadata":{"id":"Tz7rC_u-roJa"},"source":["### Creating multi-class classification data\n","\n","To begin a multi-class classification problem, let's create some multi-class data.\n","\n","To do so, we can leverage Scikit-Learn's [`make_blobs()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) method.\n","\n","This method will create however many classes (using the `centers` parameter) we want.\n","\n","Specifically, let's do the following:\n","\n","1. Create some multi-class data with `make_blobs()`.\n","2. Turn the data into tensors (the default of `make_blobs()` is to use NumPy arrays).\n","3. Split the data into training and evaluation sets using `train_test_split()`.\n","4. Visualize the data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":691},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1728995187197,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"sUHHK5ejf44v","outputId":"a9177e92-c771-4d56-84ab-d71164412e76"},"outputs":[],"source":["# Import dependencies\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import train_test_split\n","\n","# Set the hyperparameters for data creation\n","NUM_CLASSES = 4\n","NUM_FEATURES = 2\n","RANDOM_SEED = 42\n","\n","# 1. Create multi-class data\n","X_blob, y_blob = make_blobs(n_samples=1000,\n","    n_features=NUM_FEATURES, # X features\n","    centers=NUM_CLASSES, # y labels\n","    cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n","    random_state=RANDOM_SEED\n",")\n","\n","# 2. Turn data into tensors\n","X_blob = torch.from_numpy(X_blob).type(torch.float)\n","y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n","print(X_blob[:5], y_blob[:5])\n","\n","# 3. Split into train and test sets\n","X_blob_train, X_blob_eval, y_blob_train, y_blob_eval = train_test_split(X_blob,\n","    y_blob,\n","    test_size=0.2,\n","    random_state=RANDOM_SEED\n",")\n","\n","# 4. Plot data\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);"]},{"cell_type":"markdown","metadata":{"id":"7diNr8-yr3to"},"source":["**Question:** Does this dataset need non-linearity? Or could you draw a succession of straight lines to separate it?"]},{"cell_type":"markdown","metadata":{"id":"P3ixX7mqsCui"},"source":["### Building a multi-class classification model in PyTorch\n","\n","Let's create a subclass of `nn.Module` that takes in three hyperparameters:\n","* `input_features` - the number of `X` features coming into the model.\n","* `output_features` - the ideal numbers of output features we'd like (this will be equivalent to `NUM_CLASSES` or the number of classes in your multi-class classification problem).\n","* `hidden_units` - the number of hidden neurons we'd like each hidden layer to use.\n","\n","Since we're putting things together, let's setup some device agnostic code (we don't have to do this again in the same notebook, it's only a reminder).\n","\n","Then we'll create the model class using the hyperparameters above."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1728995187197,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"ic22wotIrqz1","outputId":"b79718b6-3bfe-41a6-ea73-e41731ae8391"},"outputs":[],"source":["# Create device agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f90Uu4kEsPDT"},"outputs":[],"source":["from torch import nn\n","\n","# Build model\n","class MultiClassModel(nn.Module):\n","    def __init__(self, input_features, output_features, hidden_units=8):\n","        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n","\n","        Args:\n","            input_features (int): Number of input features to the model.\n","            out_features (int): Number of output features of the model\n","              (how many classes there are).\n","            hidden_units (int): Number of hidden units between layers, default 8.\n","        \"\"\"\n","        super().__init__()\n","        self.linear_layer_stack = nn.Sequential(\n","            nn.Linear(in_features=input_features, out_features=hidden_units),\n","            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n","            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n","            # nn.ReLU(), # <- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n","            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n","        )\n","\n","    def forward(self, x):\n","        return self.linear_layer_stack(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTXTghlksLKE"},"outputs":[],"source":["model = MultiClassModel(input_features=NUM_FEATURES,\n","                    output_features=NUM_CLASSES,\n","                    hidden_units=8).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1728995187197,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"UXkkXtchsYAE","outputId":"a08ddd48-d231-4285-cdcf-854b8df1abf6"},"outputs":[],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"PwsN7rnXzGyZ"},"source":["### Creating a loss function and optimizer for a multi-class PyTorch model\n","\n","Since we're working on a multi-class classification problem, we'll use the `nn.CrossEntropyLoss()` method as our loss function.\n","\n","And we'll stick with using SGD with a learning rate of 0.1 for optimizing our `model` parameters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vfodL6XcsZhA"},"outputs":[],"source":["# Create loss and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(),\n","                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance"]},{"cell_type":"markdown","metadata":{"id":"PjGKMptszOsl"},"source":["### Getting prediction probabilities for a multi-class PyTorch model\n","\n","We've got a loss function and optimizer ready, and we're ready to train our model but before we do let's do a single forward pass with our model to see if it works."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1728995187198,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"zDjP0_hFzNJ7","outputId":"a92b23e9-0632-4010-9798-d534cc132d81"},"outputs":[],"source":["# Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\n","model(X_blob_train.to(device))[:5]"]},{"cell_type":"markdown","metadata":{"id":"TuEmEXtuzcM-"},"source":["It looks like we get one value per feature of each sample.\n","\n","Let's check the shape to confirm."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1728995187198,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"ktwFwPofzUN0","outputId":"dd29df29-f29d-42c2-910c-3602b956a801"},"outputs":[],"source":["# How many elements in a single prediction sample?\n","model(X_blob_train.to(device))[0].shape, NUM_CLASSES"]},{"cell_type":"markdown","metadata":{"id":"7DQb-7F8zrym"},"source":["Our model is predicting one value for each class that we have.\n","\n","Do you remember what the raw outputs of our model are called?\n","\n","If you guessed **logits**, you'd be correct.\n"]},{"cell_type":"markdown","metadata":{"id":"MwMt7-w2z118"},"source":["So right now our model is outputing logits but what if we wanted to figure out exactly which label is was giving the sample?\n","\n","As in, how do we go from `logits -> prediction probabilities -> prediction labels` just like we did with the binary classification problem?\n","\n","That's where the [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function) comes into play.\n","\n","The softmax function calculates the probability of each prediction class being the actual predicted class compared to all other possible classes.\n","\n","If this doesn't make sense, let's see in code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3OxRsktzlSU"},"outputs":[],"source":["# Make prediction logits with model\n","y_logits = model(X_blob_eval.to(device))\n","\n","# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\n","y_pred_probs = torch.softmax(y_logits, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":277,"status":"ok","timestamp":1728995187468,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"n4iJUR8f0DOz","outputId":"1b42e04c-68dd-4097-91dc-54f8c039cc32"},"outputs":[],"source":["print(y_logits[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1728995187468,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"akQbKZKQ0Fn6","outputId":"7baec21b-2fcb-4eff-ad04-a78f40092f2f"},"outputs":[],"source":["print(y_pred_probs[:5])"]},{"cell_type":"markdown","metadata":{"id":"UNAyxq_P0OWl"},"source":["It may still look like the outputs of the softmax function are jumbled numbers but there's a very specific thing different about each sample.\n","\n","After passing the logits through the softmax function, each individual sample now adds to 1 (or very close to)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1728995187468,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"qsC2l3kL0F-S","outputId":"709e9299-038a-4bc8-e1a8-ba6c21815e9d"},"outputs":[],"source":["# Sum the first sample output of the softmax activation function\n","torch.sum(y_pred_probs[0])"]},{"cell_type":"markdown","metadata":{"id":"8V4FiyeM0V6F"},"source":["These prediction probabilities are essentially saying how much the model *thinks* the target `X` sample (the input) maps to each class.\n","\n","Since there's one value for each class in `y_pred_probs`, the index of the *highest* value is the class the model thinks the specific data sample *most* belongs to.\n","\n","We can check which index has the highest value using `torch.argmax()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1728995187468,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"hWFP9ggj0Q6d","outputId":"efb171f9-b057-4b7b-f030-eb8d6e87d3fe"},"outputs":[],"source":["# Which class does the model think is *most* likely at the index 0 sample?\n","print(y_pred_probs[0])\n","print(torch.argmax(y_pred_probs[0]))"]},{"cell_type":"markdown","metadata":{"id":"0H9PUW4O0dVC"},"source":["You can see the output of `torch.argmax()` returns 3, so for the features (`X`) of the sample at index 0, the model is predicting that the most likely class value (`y`) is 3."]},{"cell_type":"markdown","metadata":{"id":"oORVDdV20how"},"source":["**Summary:**\n",">\n","- To summarize the above, a model's raw output is referred to as **logits**.\n",">\n","- For a multi-class classification problem, to turn the logits into **prediction probabilities**, you use the softmax activation function (`torch.softmax`).\n",">\n","- The index of the value with the highest **prediction probability** is the class number the model thinks is *most* likely given the input features for that sample (although this is a prediction, it doesn't mean it will be correct)."]},{"cell_type":"markdown","metadata":{"id":"0ms_dNtV00BX"},"source":["### Creating a training and evaluation loop for a multi-class PyTorch model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VfzDJ_5-1FuP"},"outputs":[],"source":["# Calculate accuracy (a classification metric)\n","def accuracy_fn(y_true, y_pred):\n","    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n","    acc = (correct / len(y_pred)) * 100\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1728995187468,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"ZynVJW8T0Ykv","outputId":"9f6604f6-3787-4da1-aff9-c1409b19a805"},"outputs":[],"source":["# Fit the model\n","torch.manual_seed(42)\n","\n","# Set number of epochs\n","epochs = 100\n","\n","# Put data to target device\n","X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n","X_blob_eval, y_blob_eval = X_blob_eval.to(device), y_blob_eval.to(device)\n","\n","for epoch in range(epochs):\n","    ### Training\n","    model.train()\n","\n","    # 1. Forward pass\n","    y_logits = model(X_blob_train) # model outputs raw logits\n","    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n","    # print(y_logits)\n","    # 2. Calculate loss and accuracy\n","    loss = loss_fn(y_logits, y_blob_train)\n","    acc = accuracy_fn(y_true=y_blob_train,\n","                      y_pred=y_pred)\n","\n","    # 3. Optimizer zero grad\n","    optimizer.zero_grad()\n","\n","    # 4. Loss backwards\n","    loss.backward()\n","\n","    # 5. Optimizer step\n","    optimizer.step()\n","\n","    ### Evaluation\n","    model.eval()\n","    with torch.inference_mode():\n","      # 1. Forward pass\n","      eval_logits = model(X_blob_eval)\n","      eval_pred = torch.softmax(eval_logits, dim=1).argmax(dim=1)\n","      # 2. Calculate evaluation loss and accuracy\n","      eval_loss = loss_fn(eval_logits, y_blob_eval)\n","      eval_acc = accuracy_fn(y_true=y_blob_eval,\n","                             y_pred=eval_pred)\n","\n","    # Print out what's happening\n","    if epoch % 10 == 0:\n","        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Eval Loss: {eval_loss:.5f}, Eval Acc: {eval_acc:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"3Vod0i2T1-Na"},"source":["### Making and evaluating predictions with a PyTorch multi-class model\n","\n","Let's make some predictions and visualize them."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728995187468,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"yexP2bJN1dzE","outputId":"43e03853-7c18-437e-e6c6-cbc779c06a8c"},"outputs":[],"source":["# Make predictions\n","model.eval()\n","with torch.no_grad():\n","    y_logits = model(X_blob_eval)\n","\n","# View the first 10 predictions\n","y_logits[:10]"]},{"cell_type":"markdown","metadata":{"id":"wN3S7y_K2Jux"},"source":["Alright, looks like our model's predictions are still in logit form."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728995187468,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"JVp8Fp1N2EX2","outputId":"d8308257-d0f6-4286-8dc0-df981c5fe924"},"outputs":[],"source":["# Turn predicted logits in prediction probabilities\n","y_pred_probs = torch.softmax(y_logits, dim=1)\n","\n","# Turn prediction probabilities into prediction labels\n","y_preds = y_pred_probs.argmax(dim=1)\n","\n","# Compare first 10 model preds and test labels\n","print(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_eval[:10]}\")\n","print(f\"Eval accuracy: {accuracy_fn(y_true=y_blob_eval, y_pred=y_preds)}%\")"]},{"cell_type":"markdown","metadata":{"id":"ccbiiYP72V1U"},"source":["Let's visualize them with `plot_decision_boundary()`, remember because our data is on the GPU, we'll have to move it to the CPU for use with matplotlib (`plot_decision_boundary()` does this automatically for us)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKRtmtVC2lxh"},"outputs":[],"source":["def plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n","    \"\"\"Plots decision boundaries of model predicting on X in comparison to y.\n","\n","    Source - https://madewithml.com/courses/foundations/neural-networks/ (with modifications)\n","    \"\"\"\n","    # Put everything to CPU (works better with NumPy + Matplotlib)\n","    model.to(\"cpu\")\n","    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n","\n","    # Setup prediction boundaries and grid\n","    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n","    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n","    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n","\n","    # Make features\n","    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n","\n","    # Make predictions\n","    model.eval()\n","    with torch.inference_mode():\n","        y_logits = model(X_to_pred_on)\n","\n","    # Test for multi-class or binary and adjust logits to prediction labels\n","    if len(torch.unique(y)) > 2:\n","        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # mutli-class\n","    else:\n","        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n","\n","    # Reshape preds and plot\n","    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n","    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n","    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n","    plt.xlim(xx.min(), xx.max())\n","    plt.ylim(yy.min(), yy.max())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":545},"executionInfo":{"elapsed":560,"status":"ok","timestamp":1728995188027,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"_LYF21BG2RlJ","outputId":"0a8377f1-143e-4dee-8f62-35cb735ca235"},"outputs":[],"source":["plt.figure(figsize=(12, 6))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Train\")\n","plot_decision_boundary(model, X_blob_train, y_blob_train)\n","plt.subplot(1, 2, 2)\n","plt.title(\"Eval\")\n","plot_decision_boundary(model, X_blob_eval, y_blob_eval)"]},{"cell_type":"markdown","metadata":{"id":"WERZ982-6y0L"},"source":["## Improving our Pipeline"]},{"cell_type":"markdown","metadata":{"id":"RbS9Sjn_8GuC"},"source":["### Custom Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyAc0xny7tA4"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.datasets import make_blobs\n","from sklearn.model_selection import train_test_split\n","\n","# Set the hyperparameters for data creation\n","NUM_CLASSES = 4\n","NUM_FEATURES = 2\n","RANDOM_SEED = 42\n","\n","# 1. Create multi-class data\n","X_blob, y_blob = make_blobs(n_samples=1000,\n","    n_features=NUM_FEATURES, # X features\n","    centers=NUM_CLASSES, # y labels\n","    cluster_std=1.5, # give the clusters a little shake up\n","    random_state=RANDOM_SEED\n",")\n","\n","# 2. Turn data into tensors\n","X_blob = torch.from_numpy(X_blob).type(torch.float)\n","y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n","\n","# 3. Split into train, dev, test sets\n","X_blob_train, X_blob_eval, y_blob_train, y_blob_eval = train_test_split(X_blob,\n","    y_blob,\n","    test_size=0.2,\n","    random_state=RANDOM_SEED\n",")\n","\n","X_blob_dev, X_blob_test, y_blob_dev, y_blob_test = train_test_split(X_blob_eval,\n","    y_blob_eval,\n","    test_size=0.5,\n","    random_state=RANDOM_SEED\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aa7jk4vF8E--"},"outputs":[],"source":["# Create a custom Dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1728995188028,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"4I9Ubg9r8aMZ","outputId":"2741ed78-11b0-4a8e-f0ee-406a2b6c6f84"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4qosbkmh8lWC"},"source":["### Custom Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7ftc9s-8L4U"},"outputs":[],"source":["# Create DataLoader objects\n","train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n","eval_loader = DataLoader(dataset=dev_dataset, batch_size=16, shuffle=False)\n","test_loader= DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"YCwcBJUk81Ln"},"source":["### Modularization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VbLDMtrHA5QT"},"outputs":[],"source":["# Function to create data\n","def create_data(data_config):\n","    X_blob, y_blob = make_blobs(\n","        n_samples=data_config[\"num_samples\"],\n","        n_features=data_config[\"num_features\"],\n","        centers=data_config[\"num_classes\"],\n","        cluster_std=data_config[\"cluster_std\"],\n","        random_state=data_config[\"random_seed\"]\n","    )\n","    X_blob = torch.from_numpy(X_blob).type(torch.float)\n","    y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n","    return X_blob, y_blob\n","\n","# Function to split the data into train, dev, and test sets\n","def split_data(X, y, data_config):\n","    X_train, X_eval, y_train, y_eval = train_test_split(\n","        X, y, test_size=data_config[\"train_split\"], random_state=data_config[\"random_seed\"]\n","    )\n","    X_dev, X_test, y_dev, y_test = train_test_split(\n","        X_eval, y_eval, test_size=data_config[\"dev_split\"], random_state=data_config[\"random_seed\"]\n","    )\n","    return (X_train, y_train), (X_dev, y_dev), (X_test, y_test)\n","\n","def create_dataloaders(data, batch_size, shuffle):\n","    dataset = BlobDataset(data[0], data[1])\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","    return dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QO1oIgrBapu"},"outputs":[],"source":["# Training loop with evaluation at the end of each epoch\n","def train_and_evaluate(model, train_loader, train_data, dev_loader, dev_data, loss_fn, optimizer, training_config):\n","    epochs = training_config[\"epochs\"]\n","    training_steps = 0\n","    evaluation_steps = 0\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        # Training step\n","        for X_batch, y_batch in train_loader:\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","\n","            # Optimizer zero grad\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            y_logits = model(X_batch) # model outputs raw logits\n","\n","            # Calculate loss\n","            loss = loss_fn(y_logits, y_batch)\n","\n","            # Loss backwards\n","            loss.backward()\n","\n","            # Optimizer step\n","            optimizer.step()\n","\n","            # Update training epoch loss\n","            total_loss += loss.item()\n","\n","            print(f\"Epoch: {epoch} | Step: {training_steps} | Loss: {loss.item()}\")\n","\n","            # Update training steps\n","            training_steps += 1\n","\n","        # Calculate log epoch-level loss\n","        train_loss = total_loss / len(train_loader)\n","\n","        # Calculate log epoch-level accuracy\n","        model.eval()\n","        with torch.no_grad():\n","            y_logits = model(train_data[0]) # model outputs raw logits\n","            y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n","            train_acc = accuracy_fn(y_true=train_data[1],\n","                          y_pred=y_pred)\n","\n","        print(f\"Epoch: {epoch} | Train Loss: {train_loss} | Train Accuracy: {train_acc}\")\n","\n","        # Evaluation step\n","        model.eval()\n","        total_dev_loss = 0\n","\n","        with torch.no_grad():\n","            for X_batch, y_batch in dev_loader:\n","                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","\n","                # Forward pass\n","                y_logits = model(X_batch) # model outputs raw logits\n","\n","                # Calculate loss\n","                loss = loss_fn(y_logits, y_batch)\n","\n","                # Update evalution epoch loss\n","                total_dev_loss += loss.item()\n","\n","                print(f\"Epoch: {epoch} | Evaluation Step: {evaluation_steps} | Eval Loss: {loss.item()}\")\n","\n","                # Update evaluation steps\n","                evaluation_steps += 1\n","\n","        # Calculate log epoch-level loss\n","        dev_loss = total_dev_loss / len(dev_loader)\n","\n","        # Calculate log epoch-level accuracy\n","        model.eval()\n","        with torch.no_grad():\n","            y_logits = model(dev_data[0]) # model outputs raw logits\n","            y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n","            eval_acc = accuracy_fn(y_true=dev_data[1],\n","                          y_pred=y_pred)\n","\n","        print(f\"Epoch: {epoch} | Eval Loss: {dev_loss} | Eval Accuracy: {eval_acc}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"im-D1uE_SZWs"},"outputs":[],"source":["# Test evaluation function\n","def evaluate_test_set(model, test_data):\n","\n","    ### Evaluation\n","    model.eval()\n","    with torch.inference_mode():\n","        y_logits = model(test_data[0]) # model outputs raw logits\n","        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -> prediction probabilities -> prediction labels\n","        test_acc = accuracy_fn(y_true=test_data[1],\n","                  y_pred=y_pred)\n","\n","    print(f\"Test Accuracy: {test_acc}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define all configurations\n","data_config = {\n","    \"num_samples\": 1000,\n","    \"num_classes\": 4,\n","    \"num_features\": 2,\n","    \"random_seed\": 42,\n","    \"cluster_std\": 1.5,\n","    \"train_split\": 0.2,\n","    \"dev_split\": 0.5\n","}\n","\n","model_config = {\n","    \"hidden_units\": 8,\n","    \"input_features\": data_config[\"num_features\"],\n","    \"output_features\": data_config[\"num_classes\"]\n","}\n","\n","loss_config = {\"loss_fn\": nn.CrossEntropyLoss()}\n","\n","optimizer_config = {\n","    \"lr\": 0.1,\n","    \"optimizer\": 'SGD'\n","}\n","\n","training_config = {\n","    \"epochs\": 10,\n","    \"train_batch_size\": 32,\n","    \"train_shuffle\": False,\n","    \"dev_batch_size\": 16,\n","    \"dev_shuffle\": False\n","}\n","\n","test_config = {\n","    \"test_batch_size\": 32,\n","    \"test_shuffle\": False\n","}\n","\n","config = {'data_config': data_config, 'model_config': model_config, 'optimizer_config': optimizer_config,\\\n","          'loss_config': loss_config, 'training_config': training_config, 'test_config': test_config}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create data\n","X_blob, y_blob = create_data(data_config)\n","\n","# Split data into train, dev, test sets\n","train_data, dev_data, test_data = split_data(X_blob, y_blob, data_config)\n","\n","# Create DataLoaders\n","train_loader = create_dataloaders(train_data, training_config['train_batch_size'], training_config['train_shuffle'])\n","dev_loader = create_dataloaders(dev_data, training_config['dev_batch_size'], training_config['dev_shuffle'])\n","test_loader = create_dataloaders(test_data, test_config['test_batch_size'], test_config['test_shuffle'])\n","\n","# Initialize model, loss function, and optimizer\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = MultiClassModel(**model_config).to(device)\n","loss_fn = loss_config[\"loss_fn\"]\n","optimizer = torch.optim.SGD(model.parameters(), lr=optimizer_config[\"lr\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Run training and evaluation\n","train_and_evaluate(model, train_loader, train_data, dev_loader, dev_data, loss_fn, optimizer, training_config)\n","test_accuracy = evaluate_test_set(model, test_data)"]},{"cell_type":"markdown","metadata":{"id":"Ee9gb3DGXCiu"},"source":["### Checkpoiting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6nxRBqbXCOK"},"outputs":[],"source":["# Function to save checkpoints\n","def save_checkpoint(epoch, wandb, model, optimizer, checkpoint_path):\n","    # Save model, optimizer, and other states\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, checkpoint_path)\n","\n","    # Log checkpoint to Weights & Biases as an artifact\n","    wandb.save(checkpoint_path)\n","    artifact = wandb.Artifact(f'model-checkpoint-epoch-{epoch}', type='model')\n","    artifact.add_file(checkpoint_path)\n","    wandb.log_artifact(artifact)"]},{"cell_type":"markdown","metadata":{"id":"DarYnv1_W0ZB"},"source":["### Experiment Tracking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52Qa5mXWW1CP"},"outputs":[],"source":["!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"borGaL-aXSv5"},"outputs":[],"source":["# Define all configurations\n","data_config = {\n","    \"num_samples\": 1000,\n","    \"num_classes\": 4,\n","    \"num_features\": 2,\n","    \"random_seed\": 42,\n","    \"cluster_std\": 1.5,\n","    \"train_split\": 0.2,\n","    \"dev_split\": 0.5\n","}\n","\n","model_config = {\n","    \"hidden_units\": 8,\n","    \"input_features\": data_config[\"num_features\"],\n","    \"output_features\": data_config[\"num_classes\"]\n","}\n","\n","loss_config = {\"loss_fn\": nn.CrossEntropyLoss()}\n","\n","optimizer_config = {\n","    \"lr\": 0.1,\n","    \"optimizer\": 'SGD'\n","}\n","\n","training_config = {\n","    \"epochs\": 10,\n","    \"train_batch_size\": 32,\n","    \"train_shuffle\": False,\n","    \"dev_batch_size\": 16,\n","    \"dev_shuffle\": False\n","}\n","\n","test_config = {\n","    \"test_batch_size\": 32,\n","    \"test_shuffle\": False\n","}\n","\n","config = {'data_config': data_config, 'model_config': model_config, 'optimizer_config': optimizer_config,\\\n","          'loss_config': loss_config, 'training_config': training_config, 'test_config': test_config}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728998884246,"user":{"displayName":"Tiago","userId":"14603740172974723628"},"user_tz":-60},"id":"EXI4wMd1XTab","outputId":"774e34f1-a9b9-4c67-d53f-d239f45edd09"},"outputs":[],"source":["config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COOaZEFPZ1vm"},"outputs":[],"source":["# Create data\n","X_blob, y_blob = create_data(data_config)\n","\n","# Split data into train, dev, test sets\n","train_data, dev_data, test_data = split_data(X_blob, y_blob, data_config)\n","\n","# Create DataLoaders\n","train_loader = create_dataloaders(train_data, training_config['train_batch_size'], training_config['train_shuffle'])\n","dev_loader = create_dataloaders(dev_data, training_config['dev_batch_size'], training_config['dev_shuffle'])\n","test_loader = create_dataloaders(test_data, test_config['test_batch_size'], test_config['test_shuffle'])\n","\n","# Initialize model, loss function, and optimizer\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = MultiClassModel(**model_config).to(device)\n","loss_fn = loss_config[\"loss_fn\"]\n","optimizer = torch.optim.SGD(model.parameters(), lr=optimizer_config[\"lr\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LaXzn67oBag1"},"outputs":[],"source":["import wandb\n","\n","# Initialize W&B\n","wandb.init(project=\"blob-project\", config=config)\n","\n","# Run training and evaluation\n","train_and_evaluate(model, wandb, train_loader, train_data, dev_loader, dev_data, loss_fn, optimizer, training_config)\n","test_accuracy = evaluate_test_set(model, wandb, test_data)\n","\n","# Finish W&B logging\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"b6Rtw08lS_Qe"},"source":["Predicting on your own custom data with a trained model is possible, as long as you format the data into a similar format to what the model was trained on. Make sure you take care of the three big PyTorch and deep learning errors:\n","\n","- Wrong datatypes - Your model expected torch.float32 when your data is torch.uint8.\n","- Wrong data shapes - Your model expected [batch_size, color_channels, height, width] when your data is [color_channels, height, width].\n","- Wrong devices - Your model is on the GPU but your data is on the CPU.\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOJfj/W211uiYu5kiYHFVmf","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
